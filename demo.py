# -*- coding: utf-8 -*-
"""Demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eimcNtBPG1lEJlbYH4mksRF3TmLawdbC
"""

from google.colab import drive
drive.mount('/content/gdrive')

##insert snippet for using camera on Colab

from IPython.display import display, Javascript
from google.colab.output import eval_js
from base64 import b64decode, b64encode
import numpy as np
from PIL import Image
import io
import cv2
from imutils import face_utils
import imutils
#import argparse
import time
import dlib
import os
from google.colab.patches import cv2_imshow

"""# **Camera**"""

def VideoCapture():
  js = Javascript('''
    async function create(){
      div = document.createElement('div');
      document.body.appendChild(div);

      video = document.createElement('video');
      video.setAttribute('playsinline', '');

      div.appendChild(video);

      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: "environment"}});
      video.srcObject = stream;

      await video.play();

      canvas =  document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);

      div_out = document.createElement('div');
      document.body.appendChild(div_out);
      img = document.createElement('img');
      div_out.appendChild(img);
    }

    async function capture(){
        return await new Promise(function(resolve, reject){
            pendingResolve = resolve;
            canvas.getContext('2d').drawImage(video, 0, 0);
            result = canvas.toDataURL('image/jpeg', 0.8);
            pendingResolve(result);
        })
    }

    function showimg(imgb64){
        img.src = "data:image/jpg;base64," + imgb64;
    }

  ''')
  display(js)

def byte2image(byte):
  jpeg = b64decode(byte.split(',')[1])
  im = Image.open(io.BytesIO(jpeg))
  return np.array(im)

def image2byte(image):
  image = Image.fromarray(image)
  buffer = io.BytesIO()
  image.save(buffer, 'jpeg')
  buffer.seek(0)
  x = b64encode(buffer.read()).decode('utf-8')
  return x



# Commented out IPython magic to ensure Python compatibility.
# %cd /content/gdrive/My Drive/Final/Detect_by_Yolo/face-mask-detector

landmark_detect = dlib.shape_predictor("/content/gdrive/MyDrive/Final/Detect_by_Yolo/face-mask-detector/shape_predictor_68_face_landmarks.dat")

# Variable
confidence = 0.45
threshold = 0.3

# load the class labels our YOLO model was trained on
labelsPath = os.path.sep.join(["yolov3-mask-detector", "obj.names"])
LABELS = open(labelsPath).read().strip().split("\n")

# initialize a list of colors to represent each possible class label (red and green)
COLORS = [[0,0,255],[0,255,0]]

# derive the paths to the YOLO weights and model configuration
weightsPath = os.path.sep.join(["yolov3-mask-detector", "yolov3_face_mask.weights"])
configPath = os.path.sep.join(["yolov3-mask-detector", "yolov3.cfg"])

# load our YOLO object detector 
print("[INFO] loading YOLO from disk...")
net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)

# determine only the *output* layer names that we need from YOLO
ln = net.getLayerNames()
ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]

#Set size:
(H,W)=(None, None)

"""# **Detect**"""

VideoCapture()
eval_js('create()')
while(True):
    # Capture frame-by-frame
  byte = eval_js('capture()')
  test = byte2image(byte)
  #test = cv2.cvtColor(im, cv2.COLOR_RGB2BGR)
  (H, W) = test.shape[:2]
    # Our operations on the frame come here
  gray = cv2.cvtColor(test, cv2.COLOR_BGR2GRAY)
#construct a blob from the input image and then perform a forward
# pass of the YOLO object detector, giving us our bounding boxes and
# associated probabilities
  blob = cv2.dnn.blobFromImage(test, 1 / 255.0, (832, 832), swapRB=True, crop=False)
  net.setInput(blob)
  start = time.time()
  layerOutputs = net.forward(ln) #list of 3 arrays, for each output layer.
  end = time.time()


# initialize our lists of detected bounding boxes, confidences, and
# class IDs, respectively
  boxes = []
  confidences = []
  classIDs = []

# loop over each of the layer outputs
  for output in layerOutputs:
# loop over each of the detections
    for detection in output:
# extract the class ID and confidence (i.e., probability) of
# the current object detection
      scores = detection[5:] #last 2 values in vector
      classID = np.argmax(scores)
      confi = scores[classID]
# filter out weak predictions by ensuring the detected
# probability is greater than the minimum probability
      if confi > confidence:
# scale the bounding box coordinates back relative to the
# size of the image, keeping in mind that YOLO actually
# returns the center (x, y)-coordinates of the bounding
# box followed by the boxes' width and height
        box = detection[0:4] * np.array([W, H, W, H])
        (centerX, centerY, width, height) = box.astype("int")
# use the center (x, y)-coordinates to derive the top and
# and left corner of the bounding box
        x = int(centerX - (width / 2))
        y = int(centerY - (height / 2))
# update our list of bounding box coordinates, confidences,
# and class IDs
        boxes.append([x, y, int(width), int(height)])
        confidences.append(float(confi))
        classIDs.append(classID)

# apply NMS to suppress weak, overlapping bounding
# boxes
  idxs = cv2.dnn.NMSBoxes(boxes, confidences, confidence, threshold)
  border_size=100
  border_text_color=[255,255,255]
#Add top-border to image to display stats
  test = cv2.copyMakeBorder(test, border_size,0,0,0, cv2.BORDER_CONSTANT)
#calculate count values
  filtered_classids=np.take(classIDs,idxs)

# ensure at least one detection exists
  if len(idxs) > 0:
# loop over the indexes we are keeping
    for i in idxs.flatten():
# extract the bounding box coordinates
      (x, y) = (boxes[i][0], boxes[i][1]+border_size)
      (w, h) = (boxes[i][2], boxes[i][3])
# draw a bounding box rectangle and label on the image
      color = [int(c) for c in COLORS[classIDs[i]]]
      cv2.rectangle(test, (x, y), (x + w, y + h), color, 4)
      if classIDs[i]==0:
        cv2.putText(test, "No Mask", (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255),2)
      if classIDs[i]==1:
        rect = dlib.rectangle(int(x), int(y), int(x + w),int(y + h))
        landmark = landmark_detect(gray, rect)
        landmark = face_utils.shape_to_np(landmark)
        (mStart, mEnd) = face_utils.FACIAL_LANDMARKS_IDXS["nose"]
        nose = landmark[mStart:mEnd]
        boundRect = cv2.boundingRect(nose)
        hsv = cv2.cvtColor(test[int(boundRect[1]):int(boundRect[1] + boundRect[3]),int(boundRect[0]):int(boundRect[0] + boundRect[2])], cv2.COLOR_RGB2HSV)
        sum_saturation = np.sum(hsv[:, :, 1])
        area = int(boundRect[2])*int(boundRect[3])
        avg_saturation = sum_saturation / area
        if avg_saturation>120:
          cv2.putText(test, "Wearing Wrong", (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255),2)
        else:
          cv2.putText(test, "Safe", (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255),2)
  #cv2_imshow(test)
  eval_js('showimg("{}")'.format(image2byte(test)))





